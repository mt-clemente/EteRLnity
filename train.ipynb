{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing_extensions in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from torch) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (0.14.2)\n",
      "Requirement already satisfied: pathtools in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: setproctitle in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (1.19.1)\n",
      "Requirement already satisfied: PyYAML in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install wandb\n",
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from math import log2,ceil\n",
    "import torch\n",
    "\n",
    "# -------------------- PUZZLE SETTINGS -------------------- \n",
    "\n",
    "PATH = '/home/wsl/Polymtl/H23/INF6201/Projet/Network'\n",
    "\n",
    "\n",
    "# global 2-swap gives a size 32640 neighborhood which can be too much\n",
    "# for the GPU. Capping the swapping range helps reduce the neighborhood\n",
    "# without losing connectivity.\n",
    "SWAP_RANGE = 2\n",
    "\n",
    "MAX_BSIZE = 16\n",
    "\n",
    "PADDED_SIZE = MAX_BSIZE + 2\n",
    "NORTH = 0\n",
    "SOUTH = 1\n",
    "WEST = 2\n",
    "EAST = 3\n",
    "\n",
    "GRAY = 0\n",
    "BLACK = 23\n",
    "RED = 24\n",
    "WHITE = 25\n",
    "N_COLORS = 23\n",
    "\n",
    "\n",
    "# -------------------- NETWORK SETTINGS -------------------- \n",
    "\n",
    "GAE_LAMBDA = 0.988\n",
    "ENTROPY_WEIGHT = 0.007\n",
    "VALUE_WEIGHT = .5\n",
    "HIDDEN_SIZES = [32,32,64,128]\n",
    "KERNEL_SIZES = [3,3,3,3]\n",
    "\n",
    "# -------------------- TRAINING SETTINGS -------------------- \n",
    "UNIT = torch.float\n",
    "\n",
    "ENCODING = 'binary'\n",
    "\n",
    "if ENCODING == 'binary':\n",
    "    COLOR_ENCODING_SIZE = ceil(log2(N_COLORS))\n",
    "elif ENCODING == 'ordinal':\n",
    "    COLOR_ENCODING_SIZE = 1\n",
    "elif ENCODING == 'one_hot':\n",
    "    COLOR_ENCODING_SIZE = N_COLORS\n",
    "else:\n",
    "    raise ValueError(f\"Encoding {ENCODING} not supported\")\n",
    "  \n",
    "EPOCHS = 10\n",
    "CHECKPOINT_PERIOD = 256*200\n",
    "MINIBATCH_SIZE = 256\n",
    "HORIZON = 4 * 256\n",
    "OPT_EPSILON = 1e-6\n",
    "LR = 6e-5\n",
    "GAMMA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "\n",
    "CONFIG = {\n",
    "    'encoding':ENCODING,\n",
    "    'unit':UNIT,\n",
    "    'Batch size':MINIBATCH_SIZE,\n",
    "    'Gamma':GAMMA,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "GRAY = 0\n",
    "BLACK = 23\n",
    "RED = 24\n",
    "WHITE = 25\n",
    "\n",
    "NORTH = 0\n",
    "SOUTH = 1\n",
    "WEST = 2\n",
    "EAST = 3\n",
    "\n",
    "\n",
    "class EternityPuzzle:\n",
    "\n",
    "    def __init__(self, instance_file):\n",
    "\n",
    "        with open(instance_file) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            self.board_size = int(lines[0])\n",
    "            self.n_piece = self.board_size ** 2\n",
    "            self.n_internal_connection = 2 * self.board_size * (self.board_size - 1)\n",
    "            self.n_total_connection = self.n_internal_connection + self.board_size * 4\n",
    "\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "            self.piece_list = [(int(x.split()[0]), int(x.split()[1]), int(x.split()[2]), int(x.split()[3])) for line in\n",
    "                               lines[1:] for x in line.strip().split('\\n')]\n",
    "\n",
    "            self.n_color = max(flatten(self.piece_list)) + 1\n",
    "\n",
    "            assert (len(self.piece_list) == self.n_piece)\n",
    "\n",
    "            for p in self.piece_list:\n",
    "                assert (len(p) == 4)\n",
    "\n",
    "    def generate_rotation(self, piece):\n",
    "\n",
    "        initial_shape = piece\n",
    "        rotation_90 = (piece[2], piece[3], piece[1], piece[0])\n",
    "        rotation_180 = (piece[1], piece[0], piece[3], piece[2])\n",
    "        rotation_270 = (piece[3], piece[2], piece[0], piece[1])\n",
    "\n",
    "        return [initial_shape, rotation_90, rotation_180, rotation_270]\n",
    "\n",
    "    def get_total_n_conflict(self, solution):\n",
    "\n",
    "        n_conflict = 0\n",
    "\n",
    "        for j in range(self.board_size):\n",
    "            for i in range(self.board_size):\n",
    "\n",
    "                k = self.board_size * j + i\n",
    "                k_east = self.board_size * j + (i - 1)\n",
    "                k_south = self.board_size * (j - 1) + i\n",
    "\n",
    "                if i > 0 and solution[k][WEST] != solution[k_east][EAST]:\n",
    "                    n_conflict += 1\n",
    "\n",
    "                if i == 0 and solution[k][WEST] != GRAY:\n",
    "                    n_conflict += 1\n",
    "\n",
    "                if i == self.board_size - 1 and solution[k][EAST] != GRAY:\n",
    "                    n_conflict += 1\n",
    "\n",
    "                if j > 0 and solution[k][SOUTH] != solution[k_south][NORTH]:\n",
    "                    n_conflict += 1\n",
    "\n",
    "                if j == 0 and solution[k][SOUTH] != GRAY:\n",
    "                    n_conflict += 1\n",
    "\n",
    "                if j == self.board_size - 1 and solution[k][NORTH] != GRAY:\n",
    "                    n_conflict += 1\n",
    "\n",
    "        return n_conflict\n",
    "\n",
    "    def display_solution(self, solution, output_file):\n",
    "\n",
    "        if len(solution) < self.n_piece:\n",
    "            solution = solution + [(WHITE, WHITE, WHITE, WHITE)] * (self.n_piece - len(solution))\n",
    "\n",
    "        origin = 0\n",
    "        size = self.board_size + 2\n",
    "\n",
    "        color_dict = self.build_color_dict()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        n_total_conflict = self.get_total_n_conflict(solution)\n",
    "\n",
    "        n_internal_conflict = 0\n",
    "\n",
    "        for j in range(size):  # y-axis\n",
    "            for i in range(size):  # x-axis\n",
    "                valid_draw = [0, size - 1]\n",
    "                if i in valid_draw or j in valid_draw:\n",
    "                    ax.add_patch(patches.Rectangle((i, j), i + 1, j + 1, fill=True, facecolor=color_dict[GRAY],\n",
    "                                                   edgecolor=color_dict[BLACK]))\n",
    "                else:\n",
    "                    # ax.add_patch(patches.Rectangle((i, j), i + 1, j + 1, fill=True, facecolor='white', edgecolor='k'))\n",
    "\n",
    "                    left_bot = (i, j)\n",
    "                    right_bot = (i + 1, j)\n",
    "                    right_top = (i + 1, j + 1)\n",
    "                    left_top = (i, j + 1)\n",
    "                    middle = (i + 0.5, j + 0.5)\n",
    "\n",
    "                    instructions = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "\n",
    "                    triangle_south_path = Path([left_bot, middle, right_bot, left_bot], instructions)\n",
    "                    triangle_east_path = Path([right_top, middle, right_bot, right_top], instructions)\n",
    "                    triangle_north_path = Path([right_top, middle, left_top, right_top], instructions)\n",
    "                    triangle_west_path = Path([left_bot, middle, left_top, left_bot], instructions)\n",
    "\n",
    "                    is_triangle_south_valid = True\n",
    "                    is_triangle_north_valid = True\n",
    "                    is_triangle_east_valid = True\n",
    "                    is_triangle_west_valid = True\n",
    "\n",
    "                    k = self.board_size * (j - 1) + (i - 1)\n",
    "                    k_east = self.board_size * (j - 1) + (i - 2)\n",
    "                    k_south = self.board_size * (j - 2) + (i - 1)\n",
    "\n",
    "                    if i == 1:\n",
    "                        is_triangle_west_valid = (solution[k][WEST] == GRAY)  # 1 for Gray\n",
    "                    elif i == size - 2:\n",
    "                        is_triangle_east_valid = (solution[k][EAST] == GRAY)\n",
    "                        is_triangle_west_valid = solution[k][WEST] == solution[k_east][EAST]\n",
    "                    else:\n",
    "                        is_triangle_west_valid = solution[k][WEST] == solution[k_east][EAST]\n",
    "\n",
    "                    if j == 1:\n",
    "                        is_triangle_south_valid = (solution[k][SOUTH] == GRAY)\n",
    "                    elif j == size - 2:\n",
    "                        is_triangle_north_valid = (solution[k][NORTH] == GRAY)\n",
    "                        is_triangle_south_valid = solution[k][SOUTH] == solution[k_south][NORTH]\n",
    "                    else:\n",
    "                        is_triangle_south_valid = solution[k][SOUTH] == solution[k_south][NORTH]\n",
    "\n",
    "                    patch_south = patches.PathPatch(triangle_south_path, facecolor=color_dict[solution[k][SOUTH]],\n",
    "                                                    edgecolor=color_dict[BLACK])\n",
    "\n",
    "                    patch_north = patches.PathPatch(triangle_north_path, facecolor=color_dict[solution[k][NORTH]],\n",
    "                                                    edgecolor=color_dict[BLACK])\n",
    "\n",
    "                    patch_east = patches.PathPatch(triangle_east_path, facecolor=color_dict[solution[k][EAST]],\n",
    "                                                   edgecolor=color_dict[BLACK])\n",
    "\n",
    "                    patch_west = patches.PathPatch(triangle_west_path, facecolor=color_dict[solution[k][WEST]],\n",
    "                                                   edgecolor=color_dict[BLACK])\n",
    "\n",
    "                    if not is_triangle_south_valid:\n",
    "                        line_zip = list(zip(left_bot, right_bot))\n",
    "                        line = Line2D(line_zip[0], line_zip[1], color=color_dict[RED], lw=3)\n",
    "                        ax.add_line(line)\n",
    "\n",
    "                        if j != 1:\n",
    "                            n_internal_conflict += 1\n",
    "\n",
    "                    if not is_triangle_north_valid:\n",
    "                        line_zip = list(zip(left_top, right_top))\n",
    "                        line = Line2D(line_zip[0], line_zip[1], color=color_dict[RED], lw=3)\n",
    "                        ax.add_line(line)\n",
    "\n",
    "                        if j != size - 2:\n",
    "                            n_internal_conflict += 1\n",
    "\n",
    "                    if not is_triangle_west_valid:\n",
    "                        line_zip = list(zip(left_bot, left_top))\n",
    "                        line = Line2D(line_zip[0], line_zip[1], color=color_dict[RED], lw=3)\n",
    "                        ax.add_line(line)\n",
    "\n",
    "                        if i != 1:\n",
    "                            n_internal_conflict += 1\n",
    "\n",
    "                    if not is_triangle_east_valid:\n",
    "                        line_zip = list(zip(right_bot, right_top))\n",
    "                        line = Line2D(line_zip[0], line_zip[1], color=color_dict[RED], lw=3)\n",
    "                        ax.add_line(line)\n",
    "\n",
    "                        if i != size - 2:\n",
    "                            n_internal_conflict += 1\n",
    "\n",
    "                    ax.add_patch(patch_south)\n",
    "                    ax.add_patch(patch_north)\n",
    "                    ax.add_patch(patch_east)\n",
    "                    ax.add_patch(patch_west)\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "        plt.xlim(origin, size)\n",
    "        plt.ylim(origin, size)\n",
    "\n",
    "        title = 'Eternity of size %d X %d\\n' \\\n",
    "                'Total connections: %d    Internal connections: %d\\n' \\\n",
    "                'Total Valid connections: %d     Internal valid internal connections: %d\\n' \\\n",
    "                'Total Invalid connections: %d    Internal invalid connections: %d' % \\\n",
    "                (self.board_size, self.board_size,\n",
    "                 self.n_total_connection, self.n_internal_connection,\n",
    "                 self.n_total_connection - n_total_conflict, self.n_internal_connection - n_internal_conflict,\n",
    "                 n_total_conflict, n_internal_conflict,\n",
    "                 )\n",
    "        ax.set_title(title)\n",
    "\n",
    "        plt.savefig(output_file)\n",
    "\n",
    "    def print_solution(self, solution, output_file):\n",
    "        with open(output_file, \"w\") as file:\n",
    "            file.write(str(self.get_total_n_conflict(solution)) + \"\\n\")\n",
    "            file.write(str(self.board_size))\n",
    "            for piece in solution:\n",
    "                file.write(\"\\n\")\n",
    "                for c in piece:\n",
    "                    file.write(str(c) + \" \")\n",
    "\n",
    "    def build_color_dict(self):\n",
    "\n",
    "        color_dict = {\n",
    "            GRAY: 'gray',\n",
    "            1: 'lightcoral',\n",
    "            2: 'tab:blue',\n",
    "            3: 'tab:orange',\n",
    "            4: 'tab:green',\n",
    "            5: 'gold',\n",
    "            6: 'tab:purple',\n",
    "            7: 'tab:brown',\n",
    "            8: 'tab:pink',\n",
    "            9: 'tab:olive',\n",
    "            10: 'tab:cyan',\n",
    "            11: 'deeppink',\n",
    "            12: 'blue',\n",
    "            13: 'slateblue',\n",
    "            14: 'darkslateblue',\n",
    "            15: 'darkviolet',\n",
    "            16: 'teal',\n",
    "            17: 'wheat',\n",
    "            18: 'darkkhaki',\n",
    "            19: 'indigo',\n",
    "            20: 'fuchsia',\n",
    "            21: 'lime',\n",
    "            22: 'rosybrown',\n",
    "            BLACK: 'black',\n",
    "            RED: 'tab:red',\n",
    "            WHITE: 'white'\n",
    "        }\n",
    "        return color_dict\n",
    "\n",
    "    def hash_piece(self, piece):\n",
    "        all = self.generate_rotation(piece)\n",
    "        return min(all)\n",
    "\n",
    "    def verify_solution(self,solution):\n",
    "        hash_init = sorted([self.hash_piece(p) for p in self.piece_list])\n",
    "        hash_sol = sorted([self.hash_piece(p) for p in solution])\n",
    "\n",
    "        return hash_init == hash_sol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from hashlib import sha256\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "    def __init__(self,capacity:int, n_tiles:int,bsize:int,device) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.state_buf = torch.empty((capacity,bsize,bsize,4*COLOR_ENCODING_SIZE),device=device)\n",
    "        self.act_buf = torch.empty((capacity),dtype=int,device=device)\n",
    "        self.policy_buf = torch.empty((capacity,n_tiles),device=device)\n",
    "        self.value_buf = torch.empty((capacity),device=device)\n",
    "        self.next_value_buf = torch.empty((capacity),device=device)\n",
    "        self.rew_buf = torch.empty((capacity),device=device)\n",
    "        self.final_buf = torch.empty((capacity),dtype=int,device=device)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(\n",
    "            self,\n",
    "            state,\n",
    "            action,\n",
    "            policy,\n",
    "            value,\n",
    "            next_value,\n",
    "            reward,\n",
    "            final\n",
    "            ):\n",
    "\n",
    "\n",
    "        self.state_buf[self.ptr] = state\n",
    "        self.act_buf[self.ptr] = action\n",
    "        self.policy_buf[self.ptr] = policy\n",
    "        self.value_buf[self.ptr] = value\n",
    "        self.next_value_buf[self.ptr] = next_value\n",
    "        self.rew_buf[self.ptr] = reward\n",
    "        self.final_buf[self.ptr] = final\n",
    "\n",
    "        self.ptr += 1\n",
    "        if self.ptr == self.capacity:\n",
    "            self.ptr = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.ptr != 0:\n",
    "            raise OSError(self.ptr)\n",
    "        \n",
    "        self.ptr = 0\n",
    "\n",
    "\n",
    "\n",
    "class BatchMemory:\n",
    "    def __init__(self,n_tiles:int,bsize:int,capacity:int=MINIBATCH_SIZE, ep_length:int=256,device='cpu') -> None:\n",
    "        self.capacity = capacity\n",
    "        self.ep_length = ep_length\n",
    "        self.n_tiles = n_tiles\n",
    "        self.device = device\n",
    "        self.bsize = bsize\n",
    "        self.ptr = 0\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def load(self,buff:EpisodeBuffer):\n",
    "        self.state_buf[self.ptr] = buff.state_buf\n",
    "        self.act_buf[self.ptr] = buff.act_buf\n",
    "        self.policy_buf[self.ptr] = buff.policy_buf\n",
    "        self.value_buf[self.ptr] = buff.value_buf\n",
    "        self.next_value_buf[self.ptr] = buff.next_value_buf\n",
    "        self.final_buf[self.ptr] = buff.final_buf\n",
    "        self.rew_buf[self.ptr] = buff.rew_buf\n",
    "\n",
    "        self.ptr += 1\n",
    "    \n",
    "        buff.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "\n",
    "        if self.ptr != self.capacity:\n",
    "            print(self.ptr)\n",
    "            print(Warning(f'Memory not full : {self.ptr}/{self.capacity}'))\n",
    "        self.state_buf = torch.empty((self.capacity,self.ep_length,self.bsize,self.bsize,4*COLOR_ENCODING_SIZE),device=self.device)\n",
    "        self.act_buf = torch.empty((self.capacity,self.ep_length),dtype=int,device=self.device)\n",
    "        self.policy_buf = torch.empty((self.capacity,self.ep_length,self.n_tiles),device=self.device)\n",
    "        self.value_buf = torch.empty((self.capacity,self.ep_length),device=self.device)\n",
    "        self.next_value_buf = torch.empty((self.capacity,self.ep_length),device=self.device)\n",
    "        self.rew_buf = torch.empty((self.capacity,self.ep_length),device=self.device)\n",
    "        self.final_buf = torch.empty((self.capacity,self.ep_length),dtype=int,device=self.device)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return getattr(self,key)[:self.ptr+1]\n",
    "\n",
    "\n",
    "\n",
    "class AdvantageBuffer():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.policy = None\n",
    "        self.value = None\n",
    "        self.reward = None\n",
    "\n",
    "\n",
    "\n",
    "class TabuList():\n",
    "\n",
    "    def __init__(self,size) -> None:\n",
    "        self.size = size\n",
    "        self.tabu = {}\n",
    "\n",
    "    def push(self,state:torch.Tensor, step:int):\n",
    "        key = sha256(state.cpu().numpy()).hexdigest()\n",
    "        self.tabu[key] = step + TABU_LENGTH\n",
    "    \n",
    "    def in_tabu(self,state):\n",
    "        key = sha256(state.cpu().numpy()).hexdigest()\n",
    "        return key in self.tabu.keys()\n",
    "    \n",
    "    def filter(self,step:int):\n",
    "        self.tabu = {k:v for k,v in self.tabu.items() if v > step}\n",
    "\n",
    "    def get_update(self,batch:torch.Tensor,step:int):\n",
    "\n",
    "        np_batch = batch.cpu().numpy()\n",
    "        for i in range(np_batch.shape[0]):\n",
    "\n",
    "            key = sha256(np_batch[i]).hexdigest()\n",
    "\n",
    "            if key not in self.tabu.keys():\n",
    "\n",
    "                self.push(batch[i],step)\n",
    "                return torch.from_numpy(np_batch[i]).to(device=batch.device).to(dtype=batch.dtype),i\n",
    "            \n",
    "        return None,None\n",
    "\n",
    "    def fast_foward(self):\n",
    "        vals = self.tabu.values()\n",
    "        m = min(vals)\n",
    "        print(m)\n",
    "        for k in self.tabu.keys():\n",
    "            self.tabu[k] -= m\n",
    "        \n",
    "\n",
    "class StoppingCriterion():\n",
    "\n",
    "    def __init__(self,threshold) -> None:\n",
    "        \"\"\"\n",
    "        Stopping critirion for a trajectory.\n",
    "        Internal counter is updated each step :\n",
    "         * \\+ 1 if degrading move\n",
    "         * \\- 0.5 if the new score is better than the previous one\n",
    "         * Reset to 0 if new best score\n",
    "        \"\"\"\n",
    "        self.counter = 0\n",
    "        self.prev_score = 0\n",
    "        self.eos = False\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def update(self,score,best_score):\n",
    "\n",
    "        if score > best_score:\n",
    "            self.counter = 0\n",
    "\n",
    "        elif score > self.prev_score:\n",
    "            self.counter -= 0.5\n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        self.prev_score = score\n",
    "\n",
    "        if self.counter > self.threshold:\n",
    "            self.eos = True\n",
    "\n",
    "    def is_stale(self):\n",
    "        return self.eos\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.prev_score = 0\n",
    "        self.eos = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Conv3to2d(nn.Module):\n",
    "\n",
    "    def __init__(self,kernel_size,input_channels,layer_size,device) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=layer_size,\n",
    "            kernel_size= (kernel_size,kernel_size),\n",
    "            dtype=UNIT,\n",
    "            device=device,\n",
    "            )\n",
    "        \n",
    "    def  forward(self,x):\n",
    "        x = self.conv(rearrange(x,'b c h w d -> b c h (w d)'))\n",
    "        return x\n",
    "\n",
    "class View(nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(-1,*self.shape)\n",
    "\n",
    "class SoftmaxStable(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x - x.max(dim=-1, keepdim=True).values\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes, kernel_sizes, device):\n",
    "\n",
    "        self.state_dim =state_dim\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        lin_h = self.lin_size(kernel_sizes,self.state_dim)\n",
    "        twodto3d = self.lin_size(kernel_sizes[:1],self.state_dim) * self.lin_size(kernel_sizes[:1],4 * COLOR_ENCODING_SIZE)\n",
    "        \n",
    "        lin_w = self.lin_size(kernel_sizes[1:],twodto3d)\n",
    "\n",
    "        print(state_dim)\n",
    "        lin_size = lin_h * lin_w * hidden_sizes[-1]\n",
    "\n",
    "        print(lin_h,lin_w)\n",
    "\n",
    "        print(lin_size)\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv3d(1, hidden_sizes[0],kernel_sizes[0],device=device),\n",
    "            nn.BatchNorm3d(hidden_sizes[0],device=device),\n",
    "            nn.ReLU(),\n",
    "            Conv3to2d(kernel_sizes[1],hidden_sizes[0],hidden_sizes[1],device),\n",
    "            nn.BatchNorm2d(hidden_sizes[1],device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_sizes[1], hidden_sizes[2],kernel_sizes[2],device=device),\n",
    "            nn.BatchNorm2d(hidden_sizes[2],device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_sizes[2], hidden_sizes[3],kernel_sizes[3],device=device),\n",
    "            nn.BatchNorm2d(hidden_sizes[3],device=device),\n",
    "            View((lin_size,)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin_size, 128,device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim,device=device),\n",
    "            SoftmaxStable()\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv3d(1, hidden_sizes[0],kernel_sizes[0],device=device),\n",
    "            nn.BatchNorm3d(hidden_sizes[0],device=device),\n",
    "            nn.ReLU(),\n",
    "            Conv3to2d(kernel_sizes[1],hidden_sizes[0],hidden_sizes[1],device),\n",
    "            nn.BatchNorm2d(hidden_sizes[1],device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_sizes[1], hidden_sizes[2],kernel_sizes[2],device=device),\n",
    "            nn.BatchNorm2d(hidden_sizes[2],device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_sizes[2], hidden_sizes[3],kernel_sizes[3],device=device),\n",
    "            nn.BatchNorm2d(hidden_sizes[3],device=device),\n",
    "            View((lin_size,)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin_size, 128,device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1,device=device),\n",
    "        )\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Module:\n",
    "                init.xavier_normal_(m.weight)\n",
    "\n",
    "        self.critic.apply(init_weights)\n",
    "        self.actor.apply(init_weights)\n",
    "\n",
    "    def lin_size(self, kernel_sizes, dim, strides=None):\n",
    "\n",
    "        size = dim\n",
    "\n",
    "        if strides is None:\n",
    "            strides = [1] * len(kernel_sizes)\n",
    "\n",
    "        for ks,st in zip(kernel_sizes,strides):\n",
    "            size = (size - ks) // st + 1\n",
    "\n",
    "        return size\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        policy = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return policy, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self,config):\n",
    "        self.gamma = config['gamma']\n",
    "        self.clip_eps = config['clip_eps']\n",
    "        self.lr = config['lr']\n",
    "        self.epochs = config['epochs']\n",
    "        self.minibatch_size = config['minibatch_size']\n",
    "        self.horizon = config['horizon']\n",
    "        self.state_dim = config['state_dim']\n",
    "        self.entropy_weight = config['entropy_weight']\n",
    "        self.value_weight = config['value_weight']\n",
    "        self.gae_lambda = config['gae_lambda']\n",
    "        device = config['device']\n",
    "        n_tiles = config['n_tiles']\n",
    "        hidden_sizes = config['hidden_sizes']\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "\n",
    "        self.action_dim = n_tiles\n",
    "        self.model = ActorCritic(self.state_dim, self.action_dim, hidden_sizes, kernel_sizes, device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr,weight_decay=1e-4)\n",
    "        \n",
    "    def get_action(self, policy:torch.Tensor,mask:torch.BoolTensor):\n",
    "\n",
    "        if mask.count_nonzero() == 0:\n",
    "            raise Exception(\"No playable tile\")\n",
    "        sm = torch.softmax(policy,-1) * mask\n",
    "        sm /= sm.sum(dim=-1, keepdim=True)\n",
    "        return torch.multinomial(sm,1)\n",
    "        \n",
    "    def compute_gae(self, rewards, values, next_values, finals):\n",
    "\n",
    "        td_errors = rewards + self.gamma * next_values * (1 - finals) - values\n",
    "        gae = 0\n",
    "        advantages = torch.zeros_like(td_errors)\n",
    "        for t in reversed(range(len(td_errors))):\n",
    "            gae = td_errors[t] + self.gamma * self.gae_lambda * (1 - finals[t]) * gae\n",
    "            advantages[t] = gae\n",
    "        return advantages\n",
    "    \n",
    "    # def update(self, states, actions, old_policies, values, advantages, returns):\n",
    "    def update(self,mem:BatchMemory):\n",
    "\n",
    "        advantages = self.compute_gae(\n",
    "            rewards=mem['rew_buf'],\n",
    "            values=mem['value_buf'],\n",
    "            next_values=mem['next_value_buf'],\n",
    "            finals=mem['final_buf']\n",
    "            )\n",
    "\n",
    "        returns = advantages + mem['value_buf']\n",
    "\n",
    "        \n",
    "        device = 'cuda'\n",
    "        t0 = datetime.now()\n",
    "        dataset = TensorDataset(\n",
    "            rearrange(mem['state_buf'],'b ep h w d -> (b ep) h w d').unsqueeze(1).to(device),\n",
    "            rearrange(mem['act_buf'],'b ep -> (b ep)').to(device),\n",
    "            rearrange(mem['policy_buf'],'b ep p -> (b ep) p').to(device),\n",
    "            rearrange(advantages,'b ep -> (b ep)').to(device),\n",
    "            rearrange(returns,'b ep -> (b ep)').to(device)\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.minibatch_size, shuffle=True)\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        # Perform multiple update epochs\n",
    "        for k in range(self.epochs):\n",
    "            for batch in loader:\n",
    "                batch_states, batch_actions, batch_old_policies, batch_advantages, batch_returns = batch\n",
    "\n",
    "                batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std()+1e-7)\n",
    "                batch_returns = (batch_returns - batch_returns.mean()) / (batch_returns.std()+1e-7)\n",
    "                # Calculate new policy and value estimates\n",
    "                batch_policy, batch_value = self.model(batch_states)\n",
    "                # Calculate ratios and surrogates for PPO loss\n",
    "                action_probs = batch_policy.gather(1, batch_actions.unsqueeze(1))\n",
    "                old_action_probs = batch_old_policies.gather(1, batch_actions.unsqueeze(1))\n",
    "                ratio = action_probs / (old_action_probs + 1e-6)\n",
    "                clipped_ratio = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps)\n",
    "                surrogate1 = ratio * batch_advantages.unsqueeze(1)\n",
    "                surrogate2 = clipped_ratio * batch_advantages.unsqueeze(1)\n",
    "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "                # Calculate value function loss\n",
    "                value_loss = F.mse_loss(batch_value.squeeze(), batch_returns) * self.value_weight\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = -(batch_policy[batch_policy != 0] * torch.log(batch_policy[batch_policy != 0])).sum(dim=-1).mean()\n",
    "                entropy_loss = -self.entropy_weight * entropy\n",
    "                # Compute total loss and update parameters\n",
    "\n",
    "\n",
    "                loss = policy_loss + value_loss + entropy_loss\n",
    "                if False:\n",
    "                    print(\"--------\")\n",
    "                    print(batch_actions.max())\n",
    "                    print(batch_actions.min())\n",
    "                    print(batch_advantages.max())\n",
    "                    print(batch_advantages.min())\n",
    "                    print(batch_old_policies.max())\n",
    "                    print(batch_old_policies.min())\n",
    "                    print(batch_returns.max())\n",
    "                    print(batch_returns.min())\n",
    "                    print(torch.topk(batch_policy,4).values)\n",
    "                    print(batch_policy.min())\n",
    "                    print(entropy_loss.item(),value_loss.item(),policy_loss.item(),loss)\n",
    "                    print(batch_states.max())\n",
    "                    print(batch_states.min())\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(),0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                g=0\n",
    "                for name,param in self.model.named_parameters():\n",
    "                    # print(f\"{name:>28} - {torch.norm(param.grad)}\")\n",
    "                    g+=torch.norm(param.grad)\n",
    "                    param.grad.clamp_(-1,1)\n",
    "\n",
    "            if k == self.epochs -1:\n",
    "                    \n",
    "\n",
    "                wandb.log({\n",
    "                    \"Total loss\":loss,\n",
    "                    \"Cumul grad norm\":g,\n",
    "                    \"Value loss\":value_loss,\n",
    "                    \"Entropy loss\":entropy_loss,\n",
    "                    \"Policy loss\":policy_loss,\n",
    "                    })\n",
    "\n",
    "        if  not torch.cuda.is_available():\n",
    "            self.model = self.model.cpu()\n",
    "        print(datetime.now()-t0)\n",
    "        mem.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "# -------------------- UTILS --------------------\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Instances parameters\n",
    "    parser.add_argument('--instance', type=str, default='input')\n",
    "    parser.add_argument('--hotstart', type=str, default=False)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def initialize_sol(instance_file:str, device):\n",
    "\n",
    "    pz = EternityPuzzle(instance_file)\n",
    "    n_tiles = len(pz.piece_list)\n",
    "    tiles = rearrange(to_tensor(pz.piece_list),'h w d -> (h w) d').to(device)\n",
    "    return torch.zeros((pz.board_size+2,pz.board_size+2,4*COLOR_ENCODING_SIZE),device=device), tiles, n_tiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pprint(state,bsize):\n",
    "    offset = (PADDED_SIZE - bsize) // 2\n",
    "\n",
    "    if state.size()[0] != PADDED_SIZE:\n",
    "        for s in state:\n",
    "            print(s[offset:offset+bsize,offset:offset+bsize])\n",
    "\n",
    "    else:\n",
    "        print(state[offset:offset+bsize,offset:offset+bsize])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def ucb(q,count,step):\n",
    "\n",
    "    return q + 0 * torch.sqrt(-torch.log((count + 0.1)/(step + 0.1)))\n",
    "\n",
    "def binary(x: Tensor, bits):\n",
    "    mask = 2**torch.arange(bits)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).to(UNIT)\n",
    "\n",
    "\n",
    "def to_tensor(list_sol:list, encoding = ENCODING,gray_borders:bool=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Converts solutions from list format to a torch Tensor.\n",
    "    Tensor format:\n",
    "    [MAX_BSIZE, MAX_BSIZE, N_COLORS * 4]\n",
    "    Each tile is represented as a vector, consisting of concatenated one hot encoding of the colors\n",
    "    in the order  N - S - E - W . \n",
    "    If there were 4 colors a grey tile would be :\n",
    "        N       S       E       W\n",
    "    [1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
    "    TODO: convert the list to tensor once \n",
    "    \"\"\"\n",
    "\n",
    "    # To be able to rotate tiles easily, it is better to have either NESW or NWSE\n",
    "    orientation = [0,2,1,3]\n",
    "\n",
    "    list_sol = list_sol.copy()\n",
    "    b_size = int(len(list_sol)**0.5)\n",
    "\n",
    "\n",
    "    if gray_borders:\n",
    "        sol = []\n",
    "        l = len(list_sol)\n",
    "\n",
    "        for k in range(l):\n",
    "\n",
    "            # if on the border\n",
    "            if k < b_size or k % b_size == 0 or k % (b_size) == b_size-1 or k > b_size * (b_size - 1):\n",
    "                border = True\n",
    "            \n",
    "            else:\n",
    "                border = False\n",
    "\n",
    "            for i in range(len(list_sol)):\n",
    "                \n",
    "                tile = list_sol[i]\n",
    "\n",
    "                if (border and GRAY in tile) or (not border and not (GRAY in tile)):\n",
    "                    sol.append(tile)\n",
    "                    list_sol.pop(i)\n",
    "                    break\n",
    "    else:\n",
    "        sol = list_sol\n",
    "    b_size = int(len(sol)**0.5)\n",
    "\n",
    "    if encoding == 'binary':\n",
    "        color_enc_size = ceil(torch.log2(torch.tensor(N_COLORS)))\n",
    "        tens = torch.zeros((b_size,b_size,4*color_enc_size), device='cuda' if torch.cuda.is_available() else 'cpu',dtype=UNIT)\n",
    "\n",
    "        # Tiles around the board\n",
    "        # To make sure the policy learns that the gray tiles are always one the border,\n",
    "        # the reward for connecting to those tiles is bigger.\n",
    "        tens[0,:,color_enc_size:2*color_enc_size] = binary(torch.tensor(GRAY),color_enc_size)\n",
    "        tens[:,0,2*color_enc_size:color_enc_size*3] = binary(torch.tensor(GRAY),color_enc_size)\n",
    "        tens[b_size-1,:,:color_enc_size] = binary(torch.tensor(GRAY),color_enc_size)\n",
    "        tens[:,b_size-1,3*color_enc_size:] = binary(torch.tensor(GRAY),color_enc_size)\n",
    "\n",
    "\n",
    "\n",
    "        # center the playable board as much as possible\n",
    "        #one hot encode the colors\n",
    "        for i in range(b_size):\n",
    "            for j in range(b_size):\n",
    "\n",
    "                tens[i,j,:] = 0\n",
    "\n",
    "                for d in range(4):\n",
    "                    dir = orientation[d]\n",
    "                    tens[i,j, d * color_enc_size:(d+1) * color_enc_size] = binary(torch.tensor(sol[i * b_size + j][dir]),color_enc_size)\n",
    "\n",
    "    elif encoding == 'ordinal':\n",
    "        tens = torch.zeros((b_size,b_size,4), device='cuda' if torch.cuda.is_available() else 'cpu',dtype=UNIT)\n",
    "\n",
    "        # Tiles around the board\n",
    "        # To make sure the policy learns that the gray tiles are always one the border,\n",
    "        # the reward for connecting to those tiles is bigger.\n",
    "        tens[0,:,1] = 0\n",
    "        tens[:,0,2] = 0\n",
    "        tens[b_size-1,:,0] = 0\n",
    "        tens[:,b_size-1,3] = 0\n",
    "\n",
    "\n",
    "        # center the playable board as much as possible\n",
    "        #one hot encode the colors\n",
    "        for i in range(b_size):\n",
    "            for j in range(b_size):\n",
    "\n",
    "                tens[i,j,:] = 0\n",
    "\n",
    "                for d in range(4):\n",
    "                    dir = orientation[d]\n",
    "                    tens[i,j,d] = torch.tensor(sol[i * b_size + j][dir])\n",
    "        \n",
    "        tens.unsqueeze(-1)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        tens = torch.zeros((b_size,b_size,4*N_COLORS), device='cuda' if torch.cuda.is_available() else 'cpu',dtype=UNIT)\n",
    "\n",
    "        tens[0,:,N_COLORS + GRAY] = 1\n",
    "        tens[:,0,N_COLORS * 2 + GRAY] = 1\n",
    "        tens[b_size-1,:,GRAY] = 1\n",
    "        tens[:,b_size-1,N_COLORS * 3 + GRAY] = 1\n",
    "\n",
    "\n",
    "        # center the playable board as much as possible\n",
    "        #one hot encode the colors\n",
    "        for i in range(b_size):\n",
    "            for j in range(b_size):\n",
    "\n",
    "                if i >= 0 and i < b_size and j >= 0 and j < b_size:\n",
    "                    tens[i,j,:] = 0\n",
    "\n",
    "                    for d in range(4):\n",
    "                        dir = orientation[d]\n",
    "                        tens[i,j, d * N_COLORS + sol[i * b_size + j][dir]] = 1\n",
    "                    \n",
    "                else:\n",
    "                    for dir in range(4):\n",
    "                        tens[i,j, orientation[dir] * N_COLORS] = 1\n",
    "        \n",
    "\n",
    "    return tens\n",
    "\n",
    "def base10(x:Tensor):\n",
    "    s = 0\n",
    "    for i in range(x.size()[0]):\n",
    "        s += x[i] * 2**i\n",
    "    \n",
    "    return int(s)\n",
    "\n",
    "def pprint(state,bsize):\n",
    "    offset = (MAX_BSIZE + 2 - bsize) // 2\n",
    "\n",
    "    if state.size()[0] != MAX_BSIZE + 2:\n",
    "        for s in state:\n",
    "            print(s[offset:offset+bsize,offset:offset+bsize])\n",
    "\n",
    "    else:\n",
    "        print(state[offset:offset+bsize,offset:offset+bsize])\n",
    "        \n",
    "\n",
    "\n",
    "def to_list(sol:torch.Tensor,bsize:int) -> list:\n",
    "\n",
    "    orientation = [0,2,1,3]\n",
    "\n",
    "    list_sol = []\n",
    "\n",
    "    sol.int()\n",
    "\n",
    "    offset = 1\n",
    "\n",
    "    if ENCODING == 'binary':\n",
    "\n",
    "        for i in range(offset, offset + bsize):\n",
    "            for j in range(offset, offset + bsize):\n",
    "\n",
    "                temp = [0]*4\n",
    "                for d in range(4):\n",
    "                    dir = orientation[d]\n",
    "                    temp[d] = base10(sol[i,j,dir*COLOR_ENCODING_SIZE:(dir+1)*COLOR_ENCODING_SIZE])\n",
    "                \n",
    "                list_sol.append(tuple(temp))\n",
    "    \n",
    "    elif ENCODING == 'ordinal':\n",
    "\n",
    "        for i in range(offset, offset + bsize):\n",
    "            for j in range(offset, offset + bsize):\n",
    "\n",
    "                temp = [0] * 4\n",
    "                for d in range(4):\n",
    "                    dir = orientation[d]\n",
    "                    temp[d] = sol[i,j,dir].item()\n",
    "\n",
    "                list_sol.append(tuple(temp))\n",
    "\n",
    "    if ENCODING == 'one_hot':\n",
    "\n",
    "        for i in range(offset, offset + bsize):\n",
    "            for j in range(offset, offset + bsize):\n",
    "\n",
    "                temp = [0] * 4\n",
    "\n",
    "                for d in range(4):\n",
    "                    dir = orientation[d]\n",
    "                    temp[d] = torch.where(sol[i,j,dir*COLOR_ENCODING_SIZE:(dir+1)*COLOR_ENCODING_SIZE] == 1)[0].item()\n",
    "                list_sol.append(tuple(temp))\n",
    "\n",
    "    return list_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from math import  exp\n",
    "import wandb\n",
    "\n",
    "LOG_EVERY = 1\n",
    "\n",
    "def train_model(instance:str,hotstart:str = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Your solver for the problem\n",
    "    :param eternity_puzzle: object describing the input\n",
    "    :return: a tuple (solution, cost) where solution is a list of the pieces (rotations applied) and\n",
    "        cost is the cost of the solution\n",
    "    \"\"\"\n",
    "    pz = EternityPuzzle(instance)\n",
    "    n_tiles = len(pz.piece_list)\n",
    "    bsize = pz.board_size\n",
    "    # torch.cuda.is_available = lambda : False\n",
    "    \n",
    "    # -------------------- GAME INIT --------------------\n",
    "\n",
    "    # -------------------- NETWORK INIT -------------------- \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "\n",
    "    cfg = {\n",
    "    'n_tiles' : n_tiles ,\n",
    "    'gamma' : GAMMA ,\n",
    "    'clip_eps' : CLIP_EPS ,\n",
    "    'lr' : LR ,\n",
    "    'epochs' : EPOCHS ,\n",
    "    'minibatch_size' : MINIBATCH_SIZE ,\n",
    "    'horizon' : HORIZON,\n",
    "    'state_dim' : bsize+2,\n",
    "    'hidden_sizes': HIDDEN_SIZES,\n",
    "    'kernel_sizes': KERNEL_SIZES,\n",
    "    'entropy_weight':ENTROPY_WEIGHT,\n",
    "    'value_weight':VALUE_WEIGHT,\n",
    "    'gae_lambda':GAE_LAMBDA,\n",
    "    'device' : device \n",
    "    }\n",
    "\n",
    "\n",
    "    agent = PPOAgent(cfg)\n",
    "\n",
    "    move_buffer = AdvantageBuffer()\n",
    "    \n",
    "    memory = BatchMemory(\n",
    "        n_tiles=n_tiles,\n",
    "        bsize=bsize+2,\n",
    "        ep_length=bsize**2,\n",
    "        capacity=HORIZON//n_tiles,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    ep_buf = EpisodeBuffer(\n",
    "        capacity=n_tiles,\n",
    "        n_tiles=n_tiles,\n",
    "        bsize=bsize+2,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    max_entropy = torch.log2(torch.tensor(MAX_BSIZE**2))\n",
    "    policy_entropy = max_entropy\n",
    "\n",
    "    # -------------------- TRAINING LOOP --------------------\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    print(\"start\")\n",
    "\n",
    "    best_score = 0\n",
    "    episode = 0\n",
    "\n",
    "    try:\n",
    "        \n",
    "        while 1:\n",
    "\n",
    "            state, remaining_tiles, n_tiles = initialize_sol(instance,device)\n",
    "            state = state.to(dtype=UNIT)\n",
    "            mask = (torch.zeros_like(remaining_tiles[:,0]) == 0)\n",
    "            episode_end = False\n",
    "            prev_conflicts = get_conflicts(state,bsize)\n",
    "\n",
    "            # print(f\"NEW EPISODE : - {episode:>5}\")\n",
    "            conflicts = 0\n",
    "            ep_reward = 0\n",
    "            consec_good_moves = 0\n",
    "            episode_best_score = 0\n",
    "            ep_start = step\n",
    "            ep_step = 0\n",
    "\n",
    "            if episode != 0:\n",
    "                memory.load(ep_buf)\n",
    "\n",
    "            while not episode_end:\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    policy, value = agent.model(state.unsqueeze(0).unsqueeze(0))\n",
    "                    # print(policy.max())\n",
    "                    # print(policy.min())\n",
    "\n",
    "                selected_tile_idx = agent.get_action(policy,mask)\n",
    "                \n",
    "                selected_tile = remaining_tiles[selected_tile_idx]\n",
    "\n",
    "                mask[selected_tile_idx] = False\n",
    "\n",
    "                new_state, new_conf = place_tile(state,selected_tile,ep_step)\n",
    "\n",
    "                conflicts += new_conf #get_conflicts(new_state,bsize)\n",
    "\n",
    "                if new_conf == 0:\n",
    "                    consec_good_moves += 1\n",
    "                    reward = streak(consec_good_moves,n_tiles)\n",
    "                else:\n",
    "                    consec_good_moves = 0\n",
    "                    reward = -0.3 * new_conf\n",
    "\n",
    "                ep_reward += reward\n",
    "\n",
    "                if ep_step != 0:\n",
    "\n",
    "                    ep_buf.push(\n",
    "                        state = move_buffer.state,\n",
    "                        action = move_buffer.action,\n",
    "                        policy = move_buffer.policy,\n",
    "                        value = move_buffer.value,\n",
    "                        next_value = value,\n",
    "                        reward = move_buffer.reward,\n",
    "                        final = 0\n",
    "                    )\n",
    "\n",
    "\n",
    "                if ep_step == n_tiles-1:\n",
    "\n",
    "                    # pz.display_solution(to_list(new_state,bsize),f\"{step}\")\n",
    "                    ep_buf.push(\n",
    "                        state = state,\n",
    "                        action = selected_tile_idx,\n",
    "                        policy = policy,\n",
    "                        value = value,\n",
    "                        next_value = 0,\n",
    "                        reward = move_buffer.reward,\n",
    "                        final = 1\n",
    "                    )\n",
    "                    if episode % 10 == 0:\n",
    "                        print(f\"END EPISODE {episode} - Conflicts {conflicts}/{bsize * 2 *(bsize+1)}\",end='\\r')\n",
    "                    episode_end = True\n",
    "                    episode += 1\n",
    "\n",
    "                    if episode % LOG_EVERY == 0:\n",
    "                        wandb.log({\"Mean episode reward\":ep_reward/(step - ep_start + 1e-5),'Final conflicts':conflicts})\n",
    "                    \n",
    "\n",
    "                if (step) % HORIZON == 0 and step != 0:\n",
    "\n",
    "                    agent.update(\n",
    "                        mem=memory\n",
    "                    )\n",
    "\n",
    "\n",
    "                \n",
    "                prev_conflicts = conflicts\n",
    "                move_buffer.state = state \n",
    "                move_buffer.action = selected_tile_idx \n",
    "                move_buffer.policy = policy \n",
    "                move_buffer.value = value \n",
    "                move_buffer.reward = reward \n",
    "\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    policy_prob = torch.softmax(policy,dim=-1).squeeze(-1)\n",
    "                    policy_prob = policy_prob[policy_prob != 0]\n",
    "                    policy_entropy = -(policy_prob * torch.log2(policy_prob)).sum()\n",
    "                    \n",
    "                \n",
    "\n",
    "                if step % LOG_EVERY==0:\n",
    "\n",
    "                    wandb.log(\n",
    "                        {   \n",
    "                            'Score':conflicts,\n",
    "                            'Relative policy entropy': policy_entropy/max_entropy,\n",
    "                            'Value': value,\n",
    "                            'Reward': reward,\n",
    "                        }\n",
    "                    )\n",
    "                # -------------------- MODEL OPTIMIZATION --------------------\n",
    "\n",
    "\n",
    "                # torch.cuda.empty_cache() \n",
    "\n",
    "                step += 1\n",
    "                ep_step += 1\n",
    "        \n",
    "                # checkpoint the policy net\n",
    "                if step % CHECKPOINT_PERIOD == 0:\n",
    "                    inst = instance.replace(\"instances/eternity_\",\"\")\n",
    "                    inst = inst.replace(\".txt\",\"\")\n",
    "                    try:\n",
    "                        torch.save(agent.model.state_dict(), f'models/checkpoint/{inst}/{step // CHECKPOINT_PERIOD}.pt')\n",
    "                    except:\n",
    "                        os.mkdir(f\"models/checkpoint/{inst}/\")\n",
    "                        torch.save(agent.model.state_dict(), f'models/checkpoint/{inst}/{step // CHECKPOINT_PERIOD}.pt')\n",
    "\n",
    "\n",
    "            if episode_best_score > best_score:\n",
    "                best_score = episode_best_score\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"STILL VALID :\",pz.verify_solution(to_list(state,bsize)))\n",
    "    print(best_score)\n",
    "    return \n",
    "\n",
    "\n",
    "def place_tile(state:Tensor,tile:Tensor,step:int):\n",
    "\n",
    "    state = state.clone()\n",
    "    bsize = state.size()[0] - 2\n",
    "    best_conf = 540\n",
    "    for dir in range(4):\n",
    "        tile = tile.roll(COLOR_ENCODING_SIZE,-1)\n",
    "        state[step // bsize + 1, step % bsize + 1,:] = tile\n",
    "        conf = filling_conflicts(state,bsize,step)\n",
    "        if conf < best_conf:\n",
    "            best_state=state.clone()\n",
    "            best_conf=conf\n",
    "\n",
    "    return best_state, best_conf\n",
    "\n",
    "def streak(streak_length:int, n_tiles):\n",
    "    return (2 - exp(-streak_length * 3/(0.8 * n_tiles)))\n",
    "\n",
    "\n",
    "\n",
    "def filling_conflicts(state:Tensor, bsize:int, step):\n",
    "    i = step // bsize + 1\n",
    "    j = step % bsize + 1\n",
    "    west_tile_color = state[i,j-1,3*COLOR_ENCODING_SIZE:4*COLOR_ENCODING_SIZE]\n",
    "    south_tile_color = state[i-1,j,:COLOR_ENCODING_SIZE]\n",
    "\n",
    "    west_border_color = state[i,j,1*COLOR_ENCODING_SIZE:2*COLOR_ENCODING_SIZE]\n",
    "    south_border_color = state[i,j,2*COLOR_ENCODING_SIZE:3*COLOR_ENCODING_SIZE]\n",
    "\n",
    "    conf = 0\n",
    "\n",
    "    if j == 1:\n",
    "        if not torch.all(west_border_color == 0):\n",
    "            conf += 1\n",
    "    \n",
    "    elif not torch.all(west_border_color == west_tile_color):\n",
    "        conf += 1\n",
    "\n",
    "    if i == 1:\n",
    "        if not torch.all(south_border_color == 0):\n",
    "            conf += 1\n",
    "    \n",
    "    elif not torch.all(south_border_color == south_tile_color):\n",
    "        conf += 1\n",
    "   \n",
    "   \n",
    "    if j == bsize:\n",
    "\n",
    "        east_border_color = state[i,j,3*COLOR_ENCODING_SIZE:4*COLOR_ENCODING_SIZE]\n",
    "\n",
    "        if not torch.all(east_border_color == 0):\n",
    "            conf += 1\n",
    "    \n",
    "\n",
    "    if i == bsize:\n",
    "\n",
    "        north_border_color = state[i,j,:COLOR_ENCODING_SIZE]\n",
    "        if not torch.all(north_border_color == 0):\n",
    "            conf += 1\n",
    "    \n",
    "\n",
    "    return conf\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_conflicts(state:Tensor, bsize:int, step:int = 0) -> int:\n",
    "\n",
    "    offset = 1\n",
    "    mask = torch.ones(bsize**2)\n",
    "    board = state[offset:offset+bsize,offset:offset+bsize].clone()\n",
    "    \n",
    "    extended_board = state[offset-1:offset+bsize+1,offset-1:offset+bsize+1]\n",
    "\n",
    "    n_offset = extended_board[2:,1:-1,2*COLOR_ENCODING_SIZE:3*COLOR_ENCODING_SIZE]\n",
    "    s_offset = extended_board[:-2,1:-1,:COLOR_ENCODING_SIZE]\n",
    "    w_offset = extended_board[1:-1,:-2,3*COLOR_ENCODING_SIZE:4*COLOR_ENCODING_SIZE]\n",
    "    e_offset = extended_board[1:-1,2:,COLOR_ENCODING_SIZE:2*COLOR_ENCODING_SIZE]\n",
    "\n",
    "    n_connections = board[:,:,:COLOR_ENCODING_SIZE] == n_offset\n",
    "    s_connections = board[:,:,2*COLOR_ENCODING_SIZE:3*COLOR_ENCODING_SIZE] == s_offset\n",
    "    w_connections = board[:,:,COLOR_ENCODING_SIZE: 2*COLOR_ENCODING_SIZE] == w_offset\n",
    "    e_connections = board[:,:,3*COLOR_ENCODING_SIZE: 4*COLOR_ENCODING_SIZE] == e_offset\n",
    "\n",
    "\n",
    "\n",
    "    redundant_ns = torch.logical_and(n_connections[:-1,:],s_connections[1:,:])\n",
    "    redundant_we = torch.logical_and(w_connections[:,1:],e_connections[:,:-1])\n",
    "\n",
    "    redundant_connections = torch.all(redundant_we,dim=-1).sum() + torch.all(redundant_ns,dim=-1).sum()\n",
    "\n",
    "    all = (torch.all(n_connections,dim=-1).sum() + torch.all(s_connections,dim=-1).sum() + torch.all(e_connections,dim=-1).sum() + torch.all(w_connections,dim=-1).sum())\n",
    "    \n",
    "    total_connections = all - redundant_connections\n",
    "\n",
    "    max_connections = (bsize + 1) * bsize * 2\n",
    "\n",
    "    return max_connections - total_connections\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\n",
      "W&B online. Running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "!wandb login cdd836c352ffd933807c80225c7b616d7ba369d7\n",
    "!wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1wwozc7j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-deluge-188</strong> at: <a href='https://wandb.ai/mateo-clemente/Eternity%20II/runs/1wwozc7j' target=\"_blank\">https://wandb.ai/mateo-clemente/Eternity%20II/runs/1wwozc7j</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230408_160203-1wwozc7j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1wwozc7j). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/c-mateo1/code/wandb/run-20230408_160441-8q22q7uv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mateo-clemente/Eternity%20II/runs/8q22q7uv' target=\"_blank\">chocolate-vortex-189</a></strong> to <a href='https://wandb.ai/mateo-clemente/Eternity%20II' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mateo-clemente/Eternity%20II' target=\"_blank\">https://wandb.ai/mateo-clemente/Eternity%20II</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mateo-clemente/Eternity%20II/runs/8q22q7uv' target=\"_blank\">https://wandb.ai/mateo-clemente/Eternity%20II/runs/8q22q7uv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'instances/eternity_complet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUbuntu\\home\\wsl\\Polymtl\\H23\\INF6201\\Projet\\train.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m CONFIG[\u001b[39m'\u001b[39m\u001b[39mInstance\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m instance\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m wandb\u001b[39m.\u001b[39minit(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     project\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEternity II\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     group\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistributional approach\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     config\u001b[39m=\u001b[39mCONFIG\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_model(instance)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[1;32mUbuntu\\home\\wsl\\Polymtl\\H23\\INF6201\\Projet\\train.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(instance, hotstart)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(instance:\u001b[39mstr\u001b[39m,hotstart:\u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    Your solver for the problem\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    :param eternity_puzzle: object describing the input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m    :return: a tuple (solution, cost) where solution is a list of the pieces (rotations applied) and\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m        cost is the cost of the solution\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     pz \u001b[39m=\u001b[39m EternityPuzzle(instance)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     n_tiles \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pz\u001b[39m.\u001b[39mpiece_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     bsize \u001b[39m=\u001b[39m pz\u001b[39m.\u001b[39mboard_size\n",
      "\u001b[1;32mUbuntu\\home\\wsl\\Polymtl\\H23\\INF6201\\Projet\\train.ipynb Cell 8\u001b[0m in \u001b[0;36mEternityPuzzle.__init__\u001b[0;34m(self, instance_file)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, instance_file):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(instance_file) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu/home/wsl/Polymtl/H23/INF6201/Projet/train.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(lines[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'instances/eternity_complet.txt'"
     ]
    }
   ],
   "source": [
    "# ----------- MAIN CALL -----------\n",
    "\n",
    "instance = 'instances/eternity_complet.txt'\n",
    "\n",
    "\n",
    "CONFIG['Instance'] = instance\n",
    "\n",
    "wandb.init(\n",
    "    project='Eternity II',\n",
    "    group='Distributional approach',\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "train_model(instance)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
